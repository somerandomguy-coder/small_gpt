{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "881d20b29cf04245ad02274173e6a7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14a1215c1d8f458a98af5c653b709ce3",
              "IPY_MODEL_2344f456fe5e4316bd67731535983dab",
              "IPY_MODEL_3c4db87e50624c41985431da7fbe48c2"
            ],
            "layout": "IPY_MODEL_437c5032f625496691d54e161d586828"
          }
        },
        "14a1215c1d8f458a98af5c653b709ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fcde65876164136b2f3e1993dbbdbe4",
            "placeholder": "​",
            "style": "IPY_MODEL_a653d745f780440998e78a118cc5cf82",
            "value": "Epoch 29: 100%"
          }
        },
        "2344f456fe5e4316bd67731535983dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b2b5cc9e22844458ebcd13c41b2574a",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33521c4ede74457aa815259cb60c6a42",
            "value": 65
          }
        },
        "3c4db87e50624c41985431da7fbe48c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f13cc22f0d4854a15732c47d346271",
            "placeholder": "​",
            "style": "IPY_MODEL_bc125dda9c6d4f44bbf68c4bff3d3bd5",
            "value": " 65/65 [00:05&lt;00:00, 11.06it/s, v_num=14]"
          }
        },
        "437c5032f625496691d54e161d586828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "9fcde65876164136b2f3e1993dbbdbe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a653d745f780440998e78a118cc5cf82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b2b5cc9e22844458ebcd13c41b2574a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33521c4ede74457aa815259cb60c6a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82f13cc22f0d4854a15732c47d346271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc125dda9c6d4f44bbf68c4bff3d3bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbbcbac4da8b47bbbfac49a28a3a19f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41f646d5fb4d40108f0055482a6fbef3",
              "IPY_MODEL_1a6e289d7ba2441db53158c4f7dfef82",
              "IPY_MODEL_f4803bf3a2224725a9cd2df134f8d3ee"
            ],
            "layout": "IPY_MODEL_9c8a5aa74cb54e2ab8f6569cae475ba3"
          }
        },
        "41f646d5fb4d40108f0055482a6fbef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aaa820224cb4a71aff21a27a1e935e6",
            "placeholder": "​",
            "style": "IPY_MODEL_6441f9ca87fa49f2aef239cddd6cd1eb",
            "value": "Epoch 4: 100%"
          }
        },
        "1a6e289d7ba2441db53158c4f7dfef82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ba8169cbb8c49299cedd5563f3f6af4",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf62ed45bae8483b9db19fbfc5f6783d",
            "value": 65
          }
        },
        "f4803bf3a2224725a9cd2df134f8d3ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f622666bd6b24d0cb430cbdf19803a8f",
            "placeholder": "​",
            "style": "IPY_MODEL_2508ad240c3e46088bc7fa5f35d0eb69",
            "value": " 65/65 [00:06&lt;00:00, 10.65it/s, v_num=15]"
          }
        },
        "9c8a5aa74cb54e2ab8f6569cae475ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2aaa820224cb4a71aff21a27a1e935e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6441f9ca87fa49f2aef239cddd6cd1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ba8169cbb8c49299cedd5563f3f6af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf62ed45bae8483b9db19fbfc5f6783d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f622666bd6b24d0cb430cbdf19803a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508ad240c3e46088bc7fa5f35d0eb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emoW7NOIpIrV",
        "outputId": "898d3234-c31c-4761-ef25-abef828ee0b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch-lightning)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.4.0 torchmetrics-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "6BicnAotiEPV"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "d_model = 256 #dimension of model, also the number of value coming out from the embedding value\n",
        "max_length = 65\n",
        "no_of_token = max_length\n",
        "d_ff = 1024 #dimension of feed forward layers\n",
        "num_attention_heads = 3"
      ],
      "metadata": {
        "id": "dAWvhBVL0cXi"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    sentence = sentence.split()\n",
        "    tokens = []\n",
        "    for word in sentence:\n",
        "      if word == \"<EOS>\":\n",
        "        tokens.append(word)\n",
        "      else:\n",
        "        word = word.lower()\n",
        "        if word[-1] in [\",\", \".\", \"!\", \"?\", \":\", \";\", \")\", \"]\", \"}\", \"'\", '\"']:\n",
        "          tokens.append(word[:-1])\n",
        "          tokens.append(word[-1])\n",
        "        elif word[0] in [\"(\", \"[\", \"{\", \"'\", '\"']:\n",
        "          tokens.append(word[0])\n",
        "          tokens.append(word[1:])\n",
        "        else:\n",
        "          tokens.append(word)\n",
        "    return tokens\n",
        "assert tokenize(\"i hAVe a brother, and a sister. <EOS>\") == ['i', 'have', 'a', 'brother', ',', 'and', 'a', 'sister', '.', '<EOS>']"
      ],
      "metadata": {
        "id": "OQ9cUSpfYlp-"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM-uDNCyVKbq",
        "outputId": "072c8976-0498-4ae7-f8c4-d17c9cc915fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'what': 0, 'is': 1, 'chardonnay': 2, '?': 3, '<EOS>': 4, 'a': 5, 'full-bodied': 6, 'white': 7, 'wine': 8, 'known': 9, 'for': 10, 'its': 11, 'rich': 12, ',': 13, 'creamy': 14, 'texture': 15, 'and': 16, 'flavors': 17, 'of': 18, 'butter': 19, 'vanilla': 20, 'oak': 21, '.': 22, 'it': 23, 'originates': 24, 'from': 25, 'burgundy': 26, 'france': 27, 'but': 28, 'now': 29, 'produced': 30, 'in': 31, 'many': 32, 'regions': 33, 'around': 34, 'the': 35, 'world': 36, 'pairs': 37, 'well': 38, 'with': 39, 'seafood': 40, 'dishes': 41, 'can': 42, 'you': 43, 'describe': 44, 'are': 45, 'characteristics': 46, 'where': 47, 'does': 48, 'originate': 49, 'food': 50, 'tell': 51, 'me': 52, 'about': 53, 'pinot': 54, 'noir': 55, 'light-bodied': 56, 'red': 57, 'berries': 58, 'such': 59, 'as': 60, 'cherry': 61, 'raspberry': 62, 'delicate': 63, 'aroma': 64, 'silky': 65, 'versatile': 66, 'that': 67, 'complements': 68, 'like': 69, 'roast': 70, 'chicken': 71, 'salmon': 72, 'main': 73, 'features': 74, 'how': 75, 'taste': 76, 'complement': 77, 'cabernet': 78, 'sauvignon': 79, 'robust': 80, 'blackcurrant': 81, 'cedar': 82, 'tobacco': 83, 'often': 84, 'aged': 85, 'barrels': 86, 'which': 87, 'adds': 88, 'complexity': 89, 'to': 90, 'profile': 91, 'this': 92, 'grilled': 93, 'meats': 94, 'hearty': 95, 'key': 96, 'foods': 97, 'go': 98, 'blanc': 99, 'crisp': 100, 'refreshing': 101, 'bright': 102, 'acidity': 103, 'citrus': 104, 'green': 105, 'apple': 106, 'herbs': 107, 'bordeaux': 108, 'region': 109, 'enjoyed': 110, 'salads': 111, 'found': 112, 'come': 113, 'best': 114, 'syrah': 115, 'bold': 116, 'intense': 117, 'blackberry': 118, 'pepper': 119, 'smoked': 120, 'meat': 121, 'deep': 122, 'color': 123, 'nature': 124, 'excellently': 125, 'barbecue': 126, 'flavorful': 127, 'typically': 128, 'described': 129, 'pair': 130, '2019': 131, 'domaine': 132, 'de': 133, 'la': 134, 'romanee-conti': 135, 'tache': 136, 'showcases': 137, 'an': 138, 'elegant': 139, 'bouquet': 140, 'violets': 141, 'fruit': 142, 'subtle': 143, 'notes': 144, 'earth': 145, 'spice': 146, 'on': 147, 'palate': 148, 'well-structured': 149, 'velvety': 150, 'long': 151, 'complex': 152, 'finish': 153, 'truly': 154, 'exceptional': 155, 'vintage': 156, 'tasting': 157, '2020': 158, 'bodegas': 159, 'muga': 160, 'reserva': 161, 'classic': 162, 'rioja': 163, 'aromas': 164, 'dark': 165, 'plum': 166, 'touch': 167, 'aging': 168, 'layered': 169, 'smooth': 170, 'tannins': 171, 'well-balanced': 172, 'great': 173, '2021': 174, 'meiomi': 175, 'offers': 176, 'vibrant': 177, 'nose': 178, 'ripe': 179, 'strawberry': 180, 'hints': 181, 'cinnamon': 182, 'approachable': 183, 'enjoyable': 184, 'will': 185, 'also': 186, 'age': 187, 'gracefully': 188, 'perfectly': 189, 'fresh': 190, 'goat': 191, 'cheese': 192, 'salad': 193, \"wine's\": 194, 'cuts': 195, 'through': 196, 'creaminess': 197, 'freshness': 198, 'should': 199, 'be': 200, 'paired': 201, 'good': 202, 'match': 203, 'excellent': 204, 'juicy': 205, 'steak': 206, 'or': 207, 'beef': 208, 'stew': 209, 'enhance': 210, 'richness': 211, 'ideal': 212, 'lighter': 213, 'dish': 214, 'roasted': 215, 'fillet': 216, 'choice': 217, 'balance': 218, 'pairing': 219, 'different': 220, 'oaky': 221, 'lobster': 222, 'bisque': 223, 'pasta': 224, 'alfredo': 225, 'buttery': 226, 'notable': 227, 'has': 228, 'been': 229, 'producing': 230, 'since': 231, 'roman': 232, 'times': 233, 'wines': 234, 'renowned': 235, 'their': 236, 'quality': 237, 'diversity': 238, 'blends': 239, 'merlot': 240, 'other': 241, 'varietals': 242, 'being': 243, 'particularly': 244, 'famous': 245, 'makes': 246, 'special': 247, 'significant': 248, 'napa': 249, 'valley': 250, 'california': 251, 'become': 252, 'one': 253, 'premier': 254, 'wine-producing': 255, '1970s': 256, 'warm': 257, 'climate': 258, 'diverse': 259, 'terroir': 260, 'allow': 261, 'production': 262, 'high-quality': 263, 'including': 264, 'contribute': 265, 'industry': 266, 'tuscany': 267, 'italy': 268, 'sangiovese-based': 269, 'chianti': 270, 'brunello': 271, 'di': 272, 'montalcino': 273, 'these': 274, 'celebrated': 275, 'potential': 276, 'types': 277}\n",
            "{0: 'what', 1: 'is', 2: 'chardonnay', 3: '?', 4: '<EOS>', 5: 'a', 6: 'full-bodied', 7: 'white', 8: 'wine', 9: 'known', 10: 'for', 11: 'its', 12: 'rich', 13: ',', 14: 'creamy', 15: 'texture', 16: 'and', 17: 'flavors', 18: 'of', 19: 'butter', 20: 'vanilla', 21: 'oak', 22: '.', 23: 'it', 24: 'originates', 25: 'from', 26: 'burgundy', 27: 'france', 28: 'but', 29: 'now', 30: 'produced', 31: 'in', 32: 'many', 33: 'regions', 34: 'around', 35: 'the', 36: 'world', 37: 'pairs', 38: 'well', 39: 'with', 40: 'seafood', 41: 'dishes', 42: 'can', 43: 'you', 44: 'describe', 45: 'are', 46: 'characteristics', 47: 'where', 48: 'does', 49: 'originate', 50: 'food', 51: 'tell', 52: 'me', 53: 'about', 54: 'pinot', 55: 'noir', 56: 'light-bodied', 57: 'red', 58: 'berries', 59: 'such', 60: 'as', 61: 'cherry', 62: 'raspberry', 63: 'delicate', 64: 'aroma', 65: 'silky', 66: 'versatile', 67: 'that', 68: 'complements', 69: 'like', 70: 'roast', 71: 'chicken', 72: 'salmon', 73: 'main', 74: 'features', 75: 'how', 76: 'taste', 77: 'complement', 78: 'cabernet', 79: 'sauvignon', 80: 'robust', 81: 'blackcurrant', 82: 'cedar', 83: 'tobacco', 84: 'often', 85: 'aged', 86: 'barrels', 87: 'which', 88: 'adds', 89: 'complexity', 90: 'to', 91: 'profile', 92: 'this', 93: 'grilled', 94: 'meats', 95: 'hearty', 96: 'key', 97: 'foods', 98: 'go', 99: 'blanc', 100: 'crisp', 101: 'refreshing', 102: 'bright', 103: 'acidity', 104: 'citrus', 105: 'green', 106: 'apple', 107: 'herbs', 108: 'bordeaux', 109: 'region', 110: 'enjoyed', 111: 'salads', 112: 'found', 113: 'come', 114: 'best', 115: 'syrah', 116: 'bold', 117: 'intense', 118: 'blackberry', 119: 'pepper', 120: 'smoked', 121: 'meat', 122: 'deep', 123: 'color', 124: 'nature', 125: 'excellently', 126: 'barbecue', 127: 'flavorful', 128: 'typically', 129: 'described', 130: 'pair', 131: '2019', 132: 'domaine', 133: 'de', 134: 'la', 135: 'romanee-conti', 136: 'tache', 137: 'showcases', 138: 'an', 139: 'elegant', 140: 'bouquet', 141: 'violets', 142: 'fruit', 143: 'subtle', 144: 'notes', 145: 'earth', 146: 'spice', 147: 'on', 148: 'palate', 149: 'well-structured', 150: 'velvety', 151: 'long', 152: 'complex', 153: 'finish', 154: 'truly', 155: 'exceptional', 156: 'vintage', 157: 'tasting', 158: '2020', 159: 'bodegas', 160: 'muga', 161: 'reserva', 162: 'classic', 163: 'rioja', 164: 'aromas', 165: 'dark', 166: 'plum', 167: 'touch', 168: 'aging', 169: 'layered', 170: 'smooth', 171: 'tannins', 172: 'well-balanced', 173: 'great', 174: '2021', 175: 'meiomi', 176: 'offers', 177: 'vibrant', 178: 'nose', 179: 'ripe', 180: 'strawberry', 181: 'hints', 182: 'cinnamon', 183: 'approachable', 184: 'enjoyable', 185: 'will', 186: 'also', 187: 'age', 188: 'gracefully', 189: 'perfectly', 190: 'fresh', 191: 'goat', 192: 'cheese', 193: 'salad', 194: \"wine's\", 195: 'cuts', 196: 'through', 197: 'creaminess', 198: 'freshness', 199: 'should', 200: 'be', 201: 'paired', 202: 'good', 203: 'match', 204: 'excellent', 205: 'juicy', 206: 'steak', 207: 'or', 208: 'beef', 209: 'stew', 210: 'enhance', 211: 'richness', 212: 'ideal', 213: 'lighter', 214: 'dish', 215: 'roasted', 216: 'fillet', 217: 'choice', 218: 'balance', 219: 'pairing', 220: 'different', 221: 'oaky', 222: 'lobster', 223: 'bisque', 224: 'pasta', 225: 'alfredo', 226: 'buttery', 227: 'notable', 228: 'has', 229: 'been', 230: 'producing', 231: 'since', 232: 'roman', 233: 'times', 234: 'wines', 235: 'renowned', 236: 'their', 237: 'quality', 238: 'diversity', 239: 'blends', 240: 'merlot', 241: 'other', 242: 'varietals', 243: 'being', 244: 'particularly', 245: 'famous', 246: 'makes', 247: 'special', 248: 'significant', 249: 'napa', 250: 'valley', 251: 'california', 252: 'become', 253: 'one', 254: 'premier', 255: 'wine-producing', 256: '1970s', 257: 'warm', 258: 'climate', 259: 'diverse', 260: 'terroir', 261: 'allow', 262: 'production', 263: 'high-quality', 264: 'including', 265: 'contribute', 266: 'industry', 267: 'tuscany', 268: 'italy', 269: 'sangiovese-based', 270: 'chianti', 271: 'brunello', 272: 'di', 273: 'montalcino', 274: 'these', 275: 'celebrated', 276: 'potential', 277: 'types'}\n",
            "[[0, 1, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22], [42, 43, 44, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22], [0, 45, 35, 46, 18, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22], [47, 48, 2, 49, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22], [0, 50, 37, 38, 39, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22], [0, 42, 43, 51, 52, 53, 54, 55, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22], [44, 54, 55, 22, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22], [0, 45, 35, 73, 74, 18, 54, 55, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22], [75, 48, 54, 55, 76, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22], [0, 41, 77, 54, 55, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22], [0, 1, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22], [42, 43, 44, 35, 17, 18, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22], [0, 45, 35, 96, 46, 18, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22], [75, 1, 78, 79, 85, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22], [0, 97, 98, 38, 39, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22], [0, 1, 79, 99, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22], [44, 79, 99, 22, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22], [0, 17, 45, 112, 31, 79, 99, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22], [47, 48, 79, 99, 113, 25, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22], [0, 50, 1, 114, 39, 79, 99, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22], [0, 42, 43, 51, 52, 53, 115, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22], [44, 35, 46, 18, 115, 22, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22], [0, 45, 35, 73, 17, 18, 115, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22], [75, 1, 115, 128, 129, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22], [0, 41, 130, 38, 39, 115, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22], [0, 1, 35, 131, 132, 133, 134, 135, 134, 136, 69, 3, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22], [44, 35, 131, 132, 133, 134, 135, 134, 136, 22, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22], [0, 45, 35, 157, 144, 18, 35, 131, 132, 133, 134, 135, 134, 136, 3, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22], [75, 48, 35, 131, 132, 133, 134, 135, 134, 136, 76, 3, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22], [0, 1, 35, 158, 159, 160, 161, 69, 3, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22], [44, 35, 158, 159, 160, 161, 22, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22], [0, 45, 35, 157, 144, 18, 35, 158, 159, 160, 161, 3, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22], [75, 48, 35, 158, 159, 160, 161, 76, 3, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22], [0, 1, 35, 174, 175, 54, 55, 69, 3, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22], [44, 35, 174, 175, 54, 55, 22, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22], [0, 45, 35, 17, 18, 35, 174, 175, 54, 55, 3, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22], [75, 48, 35, 174, 175, 54, 55, 76, 3, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22], [0, 41, 130, 38, 39, 79, 99, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22], [75, 199, 79, 99, 200, 201, 39, 50, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22], [0, 1, 5, 202, 50, 203, 10, 79, 99, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22], [0, 50, 68, 79, 99, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22], [0, 1, 5, 202, 203, 10, 78, 79, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22], [75, 199, 78, 79, 200, 201, 39, 50, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22], [0, 50, 37, 38, 39, 78, 79, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22], [75, 48, 78, 79, 77, 50, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22], [0, 41, 45, 212, 39, 54, 55, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22], [75, 199, 54, 55, 200, 201, 39, 50, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22], [0, 1, 5, 202, 50, 219, 10, 54, 55, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22], [75, 48, 54, 55, 77, 220, 41, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22], [0, 41, 130, 38, 39, 2, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22], [75, 199, 2, 200, 201, 39, 50, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22], [0, 1, 5, 202, 50, 203, 10, 2, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22], [0, 50, 68, 5, 12, 2, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22], [0, 1, 227, 53, 35, 108, 109, 3, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22], [44, 35, 108, 8, 109, 22, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22], [0, 45, 108, 234, 9, 10, 3, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22], [0, 246, 108, 234, 247, 3, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22], [0, 1, 248, 53, 249, 250, 3, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22], [44, 35, 249, 250, 8, 109, 22, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22], [0, 246, 249, 250, 227, 10, 8, 262, 3, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22], [75, 48, 249, 250, 265, 90, 35, 8, 266, 3, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22], [0, 1, 9, 53, 35, 267, 8, 109, 3, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22], [44, 35, 267, 8, 109, 22, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22], [0, 277, 18, 234, 45, 30, 31, 267, 3, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22], [0, 246, 267, 234, 247, 3, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22]]\n",
            "[[1, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22, 4], [43, 44, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22, 4], [45, 35, 46, 18, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22, 4], [48, 2, 49, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22, 4], [50, 37, 38, 39, 2, 3, 4, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 13, 20, 13, 16, 21, 22, 23, 24, 25, 26, 13, 27, 13, 28, 1, 29, 30, 31, 32, 33, 34, 35, 36, 22, 35, 8, 37, 38, 39, 40, 16, 14, 41, 22, 4], [42, 43, 51, 52, 53, 54, 55, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22, 4], [54, 55, 22, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22, 4], [45, 35, 73, 74, 18, 54, 55, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22, 4], [48, 54, 55, 76, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22, 4], [41, 77, 54, 55, 3, 4, 54, 55, 1, 5, 56, 57, 8, 39, 17, 18, 57, 58, 13, 59, 60, 61, 16, 62, 22, 23, 1, 9, 10, 11, 63, 64, 16, 65, 15, 22, 54, 55, 1, 5, 66, 8, 67, 68, 41, 69, 70, 71, 16, 72, 22, 4], [1, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22, 4], [43, 44, 35, 17, 18, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22, 4], [45, 35, 96, 46, 18, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22, 4], [1, 78, 79, 85, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22, 4], [97, 98, 38, 39, 78, 79, 3, 4, 78, 79, 1, 5, 6, 57, 8, 39, 80, 17, 18, 81, 13, 82, 13, 16, 83, 22, 23, 1, 84, 85, 31, 21, 86, 13, 87, 88, 89, 90, 11, 91, 22, 92, 8, 37, 38, 39, 93, 94, 16, 95, 41, 22, 4], [1, 79, 99, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22, 4], [79, 99, 22, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22, 4], [17, 45, 112, 31, 79, 99, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22, 4], [48, 79, 99, 113, 25, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22, 4], [50, 1, 114, 39, 79, 99, 3, 4, 79, 99, 1, 5, 100, 13, 101, 7, 8, 39, 102, 103, 16, 17, 18, 104, 13, 105, 106, 13, 16, 107, 22, 23, 24, 25, 35, 108, 109, 18, 27, 16, 1, 84, 110, 39, 111, 16, 40, 22, 4], [42, 43, 51, 52, 53, 115, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22, 4], [35, 46, 18, 115, 22, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22, 4], [45, 35, 73, 17, 18, 115, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22, 4], [1, 115, 128, 129, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22, 4], [41, 130, 38, 39, 115, 3, 4, 115, 1, 5, 116, 57, 8, 39, 117, 17, 18, 118, 13, 119, 13, 16, 120, 121, 22, 23, 1, 9, 10, 11, 122, 123, 16, 6, 124, 22, 115, 37, 125, 39, 126, 16, 12, 13, 127, 41, 22, 4], [1, 35, 131, 132, 133, 134, 135, 134, 136, 69, 3, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22, 4], [35, 131, 132, 133, 134, 135, 134, 136, 22, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22, 4], [45, 35, 157, 144, 18, 35, 131, 132, 133, 134, 135, 134, 136, 3, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22, 4], [48, 35, 131, 132, 133, 134, 135, 134, 136, 76, 3, 4, 35, 131, 132, 133, 134, 135, 134, 136, 137, 138, 139, 140, 18, 141, 16, 57, 142, 39, 143, 144, 18, 145, 16, 146, 22, 147, 35, 148, 13, 23, 1, 149, 39, 5, 150, 15, 16, 5, 151, 13, 152, 153, 22, 5, 154, 155, 156, 22, 4], [1, 35, 158, 159, 160, 161, 69, 3, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22, 4], [35, 158, 159, 160, 161, 22, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22, 4], [45, 35, 157, 144, 18, 35, 158, 159, 160, 161, 3, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22, 4], [48, 35, 158, 159, 160, 161, 76, 3, 4, 35, 158, 159, 160, 161, 1, 5, 162, 163, 39, 164, 18, 165, 61, 13, 166, 13, 16, 5, 167, 18, 20, 25, 21, 168, 22, 35, 76, 1, 12, 16, 169, 13, 39, 170, 171, 16, 5, 172, 103, 22, 5, 173, 8, 10, 168, 22, 4], [1, 35, 174, 175, 54, 55, 69, 3, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22, 4], [35, 174, 175, 54, 55, 22, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22, 4], [45, 35, 17, 18, 35, 174, 175, 54, 55, 3, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22, 4], [48, 35, 174, 175, 54, 55, 76, 3, 4, 35, 174, 175, 54, 55, 176, 5, 177, 178, 18, 179, 61, 13, 180, 13, 16, 181, 18, 182, 22, 35, 148, 1, 65, 39, 17, 18, 57, 142, 13, 20, 13, 16, 5, 167, 18, 21, 22, 23, 1, 183, 16, 184, 29, 28, 185, 186, 187, 188, 22, 4], [41, 130, 38, 39, 79, 99, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22, 4], [199, 79, 99, 200, 201, 39, 50, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22, 4], [1, 5, 202, 50, 203, 10, 79, 99, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22, 4], [50, 68, 79, 99, 3, 4, 5, 100, 79, 99, 37, 189, 39, 190, 191, 192, 16, 5, 105, 193, 22, 35, 194, 103, 195, 196, 35, 197, 18, 35, 192, 16, 68, 35, 198, 18, 35, 193, 22, 4], [1, 5, 202, 203, 10, 78, 79, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22, 4], [199, 78, 79, 200, 201, 39, 50, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22, 4], [50, 37, 38, 39, 78, 79, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22, 4], [48, 78, 79, 77, 50, 3, 4, 5, 80, 78, 79, 1, 138, 204, 203, 10, 5, 205, 206, 207, 5, 95, 208, 209, 22, 35, 194, 171, 16, 116, 17, 210, 35, 211, 18, 35, 121, 22, 4], [41, 45, 212, 39, 54, 55, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22, 4], [199, 54, 55, 200, 201, 39, 50, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22, 4], [1, 5, 202, 50, 219, 10, 54, 55, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22, 4], [48, 54, 55, 77, 220, 41, 3, 4, 10, 5, 213, 214, 69, 215, 71, 207, 5, 72, 216, 13, 5, 54, 55, 1, 138, 212, 217, 22, 11, 63, 17, 16, 103, 218, 38, 39, 35, 17, 18, 35, 214, 22, 4], [41, 130, 38, 39, 2, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22, 4], [199, 2, 200, 201, 39, 50, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22, 4], [1, 5, 202, 50, 203, 10, 2, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22, 4], [50, 68, 5, 12, 2, 3, 4, 5, 12, 13, 221, 2, 68, 14, 41, 59, 60, 222, 223, 207, 5, 224, 225, 22, 35, 194, 226, 144, 210, 35, 211, 18, 35, 214, 22, 4], [1, 227, 53, 35, 108, 109, 3, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22, 4], [35, 108, 8, 109, 22, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22, 4], [45, 108, 234, 9, 10, 3, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22, 4], [246, 108, 234, 247, 3, 4, 35, 108, 109, 18, 27, 228, 229, 230, 8, 231, 232, 233, 22, 108, 234, 45, 235, 10, 236, 237, 16, 238, 13, 39, 239, 18, 78, 79, 13, 240, 13, 16, 241, 242, 243, 244, 245, 22, 4], [1, 248, 53, 249, 250, 3, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22, 4], [35, 249, 250, 8, 109, 22, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22, 4], [246, 249, 250, 227, 10, 8, 262, 3, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22, 4], [48, 249, 250, 265, 90, 35, 8, 266, 3, 4, 35, 249, 250, 31, 251, 228, 252, 253, 18, 35, 254, 255, 33, 31, 35, 36, 231, 35, 256, 22, 11, 257, 258, 16, 259, 260, 261, 10, 35, 262, 18, 263, 234, 13, 264, 78, 79, 16, 2, 22, 4], [1, 9, 53, 35, 267, 8, 109, 3, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22, 4], [35, 267, 8, 109, 22, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22, 4], [277, 18, 234, 45, 30, 31, 267, 3, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22, 4], [246, 267, 234, 247, 3, 4, 31, 268, 13, 35, 267, 109, 1, 9, 10, 11, 269, 234, 13, 244, 270, 16, 271, 272, 273, 22, 274, 234, 45, 275, 10, 236, 12, 17, 16, 168, 276, 22, 4]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(\"/content/QnA_pair.txt\", \"r\") as data:\n",
        "  data = data.readlines()\n",
        "token_to_idx = {}\n",
        "for line in data:\n",
        "  for word in tokenize(line):\n",
        "    if word not in token_to_idx:\n",
        "      token_to_idx[word] = len(token_to_idx)\n",
        "print(token_to_idx)\n",
        "idx_to_token = dict(map(reversed, token_to_idx.items()))\n",
        "print(idx_to_token)\n",
        "\n",
        "inputs = []\n",
        "labels = []\n",
        "\n",
        "for line in data:\n",
        "  EOS = 0\n",
        "  #line = What is Chardonnay? <EOS> Chardonnay is a ... <EOS>\n",
        "  input = []\n",
        "  output = []\n",
        "  #get input:\n",
        "  for word in tokenize(line):\n",
        "    if word == \"<EOS>\":\n",
        "      if EOS == 0: #if encounter the first <EOS> keep going until meet the second one\n",
        "        EOS =+ 1\n",
        "        input.append(token_to_idx[word])\n",
        "      elif EOS == 1:\n",
        "        break\n",
        "    else:\n",
        "      input.append(token_to_idx[word])\n",
        "  #get output\n",
        "  for word in tokenize(line)[1:]:\n",
        "      output.append(token_to_idx[word])\n",
        "\n",
        "  inputs.append(input)\n",
        "  labels.append(output)\n",
        "inputs.pop()\n",
        "labels.pop()\n",
        "\n",
        "print(inputs)\n",
        "print(labels)\n",
        "assert len(inputs) == len(labels)\n",
        "for i in inputs:\n",
        "  assert i.count(4) == 1 #check if in input have only one EOS\n",
        "for i in labels:\n",
        "  assert i.count(4) == 2 #check if in labels have only two EOS\n",
        "\n",
        "# Open the file for writing\n",
        "with open('file.txt', 'w') as f:\n",
        "    f.write(f\"token_to_idx: {token_to_idx}\\n\\n\")\n",
        "    f.write(f\"idx_to_token: {idx_to_token}\\n\\n\")\n",
        "    f.write(f\"inputs: {inputs}\\n\\n\")\n",
        "    f.write(f\"labels: {labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combine = inputs + labels\n",
        "print(len(inputs), len(labels))\n",
        "print(len(combine))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uQ7ue0Bjk7q",
        "outputId": "d01b8cbe-5355-417f-8285-6420eec5dedb"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 65\n",
            "130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combine = inputs+labels\n",
        "length = [len(sentence) for sentence in combine]\n",
        "print(max(length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2pGlei6kOzC",
        "outputId": "f47ee4df-1a77-4239-9473-ca20cd18e9ae"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(ins, labs):\n",
        "  combine = ins+labs\n",
        "  length = [len(sentence) for sentence in combine]\n",
        "  max_length = 65\n",
        "  padded_ins = []\n",
        "  padded_labs = []\n",
        "  for sequence in ins:\n",
        "    padding = [0] * (max_length - len(sequence))\n",
        "    padded_sequence = sequence + padding\n",
        "    padded_ins.append(padded_sequence)\n",
        "  for sequence in labs:\n",
        "    padding = [0] * (max_length - len(sequence))\n",
        "    padded_sequence = sequence + padding\n",
        "    padded_labs.append(padded_sequence)\n",
        "  return padded_ins, padded_labs\n",
        "\n",
        "one, two = pad_sequences([[3,2,4,2,3,2,3], [3,2,2]],\n",
        "                         [[3,2],[3,2,1]])\n",
        "assert len(one) == len(two)\n"
      ],
      "metadata": {
        "id": "C8X4n4bSjV8a"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = pad_sequences(inputs, labels)\n",
        "inputs = torch.tensor(inputs)\n",
        "labels = torch.tensor(labels)\n",
        "print(inputs.shape)\n",
        "print(labels.shape)\n",
        "#(no of sentence, max_lenght of each sentence)\n",
        "\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz2vovzph_Zy",
        "outputId": "58727dda-a4e0-452b-f636-230a4360da80"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([65, 65])\n",
            "torch.Size([65, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.embedding"
      ],
      "metadata": {
        "id": "bzFfB_aoxitJ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.zeros(10, 5)\n",
        "print(a) #max_len=10, d_model=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJygVeCm3NY0",
        "outputId": "a698a8ce-4c4d-4130-abc3-bb8e9860d9ff"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.size())\n",
        "print(a.size(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCfzPTU1lK-N",
        "outputId": "1de1b31e-03aa-49bf-86c4-1edc3b3d53e0"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class positional_encoding(nn.Module):\n",
        "  def __init__(self, d_model, max_length):\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(max_length, d_model)\n",
        "\n",
        "    position = torch.arange(0, max_length, step=1).float().unsqueeze(1)\n",
        "    embedded_index = torch.arange(0, d_model, step=2).float() #i    #step = 2 because i = 0 can be use for sin and cos\n",
        "\n",
        "    div_term = 1/10000 ** (embedded_index / d_model)\n",
        "    # fill in the zeros table\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward(self, word_embedding): #take output from word_embedding then calculate #over\n",
        "    return word_embedding + self.pe[:word_embedding.size(0)] # just self.pe since all inputs are already uniform due to padding\n"
      ],
      "metadata": {
        "id": "XRtJTkrdzfiV"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.w_Q = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "    self.w_K = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "    self.w_V = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "\n",
        "    self.row_dim = 0\n",
        "    self.col_dim = 1\n",
        "\n",
        "  def forward(self, encodings_Q, encodings_K, encodings_V, mask=None):\n",
        "    Q = self.w_Q(encodings_Q) #self.w_Q, w_K, w_V is a linear layer with the formula: out = in*W + b (the layer already include W and b to train itself)\n",
        "    K = self.w_K(encodings_K)\n",
        "    V = self.w_V(encodings_V)\n",
        "\n",
        "    sims = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "    scaled_sims = sims / torch.tensor(K.size(-1)).sqrt()\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_sims = scaled_sims.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scaled_sims, dim=-1) # which dimension should the weight after softmax add up to 1 in this case the sum of all column in one row equal 1\n",
        "\n",
        "    attention_score = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_score\n",
        "\n",
        "    return torch.matmul(scaled_sims, V)\n"
      ],
      "metadata": {
        "id": "_n33JS5ciI7f"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_only_transformer(pl.LightningModule):\n",
        "  def __init__(self, no_of_token, d_model, max_length, num_attention_heads=3):\n",
        "    super().__init__()\n",
        "    self.em = nn.Embedding(no_of_token, d_model)\n",
        "    self.pe = positional_encoding(d_model, max_length)\n",
        "\n",
        "    self.attention = Attention(d_model)\n",
        "    self.attention_2 = Attention(d_model=d_model)\n",
        "    self.attention_3 = Attention(d_model=d_model)\n",
        "\n",
        "    self.reduce_attention_dim = nn.Linear(in_features=(d_model), out_features=d_model)\n",
        "\n",
        "\n",
        "\n",
        "    self.fc = nn.Linear(d_model, d_model)\n",
        "    self.final_fc = nn.Linear(d_model, no_of_token)\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, token_ids):  #input is token_ids not the token itself e.g. \"4\" instead of <EOS>\n",
        "    word_embedding = self.em(token_ids)\n",
        "    position_encoding = self.pe(word_embedding)\n",
        "\n",
        "    mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
        "\n",
        "    mask = mask == 0\n",
        "\n",
        "\n",
        "    #first decoder\n",
        "    attention_score = self.attention(position_encoding, position_encoding, position_encoding, mask=mask)\n",
        "    attention_score_2 = self.attention_2(attention_score, attention_score, attention_score, mask=mask)\n",
        "    attention_score_3 = self.attention_3(attention_score_2, attention_score_2, attention_score_2, mask=mask)\n",
        "    attention_score_3 = self.reduce_attention_dim(attention_score_3)\n",
        "    residual_value = attention_score_3 + position_encoding\n",
        "    output1 = self.fc(residual_value)\n",
        "\n",
        "    #second decoder (same as above but the input is output from the previous decoder instead of the position value)\n",
        "    attention_score = self.attention(output1, output1, output1, mask=mask)\n",
        "    attention_score_2 = self.attention_2(attention_score, attention_score, attention_score, mask=mask)\n",
        "    attention_score_3 = self.attention_3(attention_score_2, attention_score_2, attention_score_2, mask=mask)\n",
        "    attention_score_3 = self.reduce_attention_dim(attention_score_3)\n",
        "    residual_value = attention_score_3 + position_encoding\n",
        "    output2 = self.fc(residual_value)\n",
        "\n",
        "    #third decoder\n",
        "\n",
        "    attention_score = self.attention(output2, output2, output2, mask=mask)\n",
        "    attention_score_2 = self.attention_2(attention_score, attention_score, attention_score, mask=mask)\n",
        "    attention_score_3 = self.attention_3(attention_score_2, attention_score_2, attention_score_2, mask=mask)\n",
        "\n",
        "    attention_score_3 = self.reduce_attention_dim(attention_score_3)\n",
        "\n",
        "    residual_value = attention_score_3 + position_encoding\n",
        "    output3 = self.fc(residual_value)\n",
        "\n",
        "    #fourth decoder\n",
        "\n",
        "    attention_score = self.attention(output3, output3, output3, mask=mask)\n",
        "    attention_score_2 = self.attention_2(attention_score, attention_score, attention_score, mask=mask)\n",
        "    attention_score_3 = self.attention_3(attention_score_2, attention_score_2, attention_score_2, mask=mask)\n",
        "\n",
        "    attention_score_3 = self.reduce_attention_dim(attention_score_3)\n",
        "\n",
        "    residual_value = attention_score_3 + position_encoding\n",
        "    output4 = self.fc(residual_value)\n",
        "\n",
        "    #fifth decoder\n",
        "\n",
        "    attention_score = self.attention(output4, output4, output4, mask=mask)\n",
        "    attention_score_2 = self.attention_2(attention_score, attention_score, attention_score, mask=mask)\n",
        "    attention_score_3 = self.attention_3(attention_score_2, attention_score_2, attention_score_2, mask=mask)\n",
        "    attention_score_3 = self.reduce_attention_dim(attention_score_3)\n",
        "\n",
        "    residual_value = attention_score_3 + position_encoding\n",
        "    output5 = self.fc(residual_value)\n",
        "\n",
        "    #sixth decoder\n",
        "\n",
        "    attention_score = self.attention(output5, output5, output5, mask=mask)\n",
        "    attention_score_2 = self.attention_2(attention_score, attention_score, attention_score, mask=mask)\n",
        "    attention_score_3 = self.attention_3(attention_score_2, attention_score_2, attention_score_2, mask=mask)\n",
        "\n",
        "    attention_score_3 = self.reduce_attention_dim(attention_score_3)\n",
        "\n",
        "    residual_value = attention_score_3 + position_encoding\n",
        "    output = self.final_fc(residual_value)\n",
        "    output = torch.clamp(output, min=0, max=self.em.num_embeddings - 1)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return Adam(self.parameters(), lr=0.0005)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    inputs, labels = batch\n",
        "    inputs = torch.clamp(inputs, min=0, max=self.em.num_embeddings - 1)\n",
        "    outputs = self.forward(inputs)\n",
        "\n",
        "    labels = torch.clamp(labels, min=0, max=self.em.num_embeddings - 1)\n",
        "    loss = self.loss(outputs.view(-1, self.em.num_embeddings), labels.view(-1))\n",
        "    self.log(\"train_loss\", loss)\n",
        "    return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6SKyGjz3iJnZ"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder_only_transformer(no_of_token=65, d_model=d_model, max_length=max_length)\n",
        "\n",
        "model_input = torch.tensor([token_to_idx[\"what\"],\n",
        "                            token_to_idx[\"is\"],\n",
        "                            token_to_idx[\"chardonnay\"],\n",
        "                            token_to_idx[\"?\"],\n",
        "                            token_to_idx[\"<EOS>\"]])\n",
        "\n",
        "input_length = model_input.size(dim=0)\n",
        "\n",
        "prediction = model(model_input)\n",
        "predict_id = torch.tensor([torch.argmax(prediction[-1, :])])\n",
        "predict_ids = predict_id #right now only 1 word was generated\n",
        "\n",
        "for i in range(max_length):\n",
        "  if predict_id == token_to_idx[\"<EOS>\"]:\n",
        "    break\n",
        "  model_input = torch.cat((model_input, predict_id))\n",
        "  prediction = model(predict_id)\n",
        "  predict_id = torch.tensor([torch.argmax(prediction[-1, :])])\n",
        "  predict_ids = torch.cat((predict_ids, predict_id))\n",
        "\n",
        "print(\"predict_ids\")\n",
        "for i in predict_ids:\n",
        "  print(idx_to_token[i.item()])"
      ],
      "metadata": {
        "id": "K6-dV01AiI4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ea285b-310c-4dff-b119-24e167424405"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict_ids\n",
            "france\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n",
            "known\n",
            "delicate\n",
            "berries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(max_epochs=30)\n",
        "trainer.fit(model, dataloader)"
      ],
      "metadata": {
        "id": "VSUTo3MIiI1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "881d20b29cf04245ad02274173e6a7f7",
            "14a1215c1d8f458a98af5c653b709ce3",
            "2344f456fe5e4316bd67731535983dab",
            "3c4db87e50624c41985431da7fbe48c2",
            "437c5032f625496691d54e161d586828",
            "9fcde65876164136b2f3e1993dbbdbe4",
            "a653d745f780440998e78a118cc5cf82",
            "0b2b5cc9e22844458ebcd13c41b2574a",
            "33521c4ede74457aa815259cb60c6a42",
            "82f13cc22f0d4854a15732c47d346271",
            "bc125dda9c6d4f44bbf68c4bff3d3bd5"
          ]
        },
        "outputId": "36a83c81-ec1f-404a-85bf-625470ba935e"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                 | Type                | Params | Mode \n",
            "---------------------------------------------------------------------\n",
            "0 | em                   | Embedding           | 16.6 K | train\n",
            "1 | pe                   | positional_encoding | 0      | train\n",
            "2 | attention            | Attention           | 197 K  | train\n",
            "3 | attention_2          | Attention           | 197 K  | train\n",
            "4 | attention_3          | Attention           | 197 K  | train\n",
            "5 | reduce_attention_dim | Linear              | 65.8 K | train\n",
            "6 | fc                   | Linear              | 65.8 K | train\n",
            "7 | final_fc             | Linear              | 16.7 K | train\n",
            "8 | loss                 | CrossEntropyLoss    | 0      | train\n",
            "---------------------------------------------------------------------\n",
            "757 K     Trainable params\n",
            "0         Non-trainable params\n",
            "757 K     Total params\n",
            "3.028     Total estimated model params size (MB)\n",
            "18        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "881d20b29cf04245ad02274173e6a7f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(max_epochs=5)\n",
        "trainer.fit(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "dbbcbac4da8b47bbbfac49a28a3a19f1",
            "41f646d5fb4d40108f0055482a6fbef3",
            "1a6e289d7ba2441db53158c4f7dfef82",
            "f4803bf3a2224725a9cd2df134f8d3ee",
            "9c8a5aa74cb54e2ab8f6569cae475ba3",
            "2aaa820224cb4a71aff21a27a1e935e6",
            "6441f9ca87fa49f2aef239cddd6cd1eb",
            "1ba8169cbb8c49299cedd5563f3f6af4",
            "bf62ed45bae8483b9db19fbfc5f6783d",
            "f622666bd6b24d0cb430cbdf19803a8f",
            "2508ad240c3e46088bc7fa5f35d0eb69"
          ]
        },
        "id": "kAG3cTql597S",
        "outputId": "13ea8706-399d-42fd-af6d-77c16f12a48d"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                 | Type                | Params | Mode \n",
            "---------------------------------------------------------------------\n",
            "0 | em                   | Embedding           | 16.6 K | train\n",
            "1 | pe                   | positional_encoding | 0      | train\n",
            "2 | attention            | Attention           | 197 K  | train\n",
            "3 | attention_2          | Attention           | 197 K  | train\n",
            "4 | attention_3          | Attention           | 197 K  | train\n",
            "5 | reduce_attention_dim | Linear              | 65.8 K | train\n",
            "6 | fc                   | Linear              | 65.8 K | train\n",
            "7 | final_fc             | Linear              | 16.7 K | train\n",
            "8 | loss                 | CrossEntropyLoss    | 0      | train\n",
            "---------------------------------------------------------------------\n",
            "757 K     Trainable params\n",
            "0         Non-trainable params\n",
            "757 K     Total params\n",
            "3.028     Total estimated model params size (MB)\n",
            "18        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbbcbac4da8b47bbbfac49a28a3a19f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_input = torch.tensor([token_to_idx[\"what\"],\n",
        "                            token_to_idx[\"is\"],\n",
        "                            token_to_idx[\"chardonnay\"],\n",
        "                            token_to_idx[\"?\"],\n",
        "                            token_to_idx[\"<EOS>\"]])\n",
        "\n",
        "input_length = model_input.size(dim=0)\n",
        "\n",
        "prediction = model(model_input)\n",
        "predict_id = torch.tensor([torch.argmax(prediction[-1, :])])\n",
        "predict_ids = predict_id #right now only 1 word was generated\n",
        "\n",
        "for i in range(max_length):\n",
        "  if predict_id == token_to_idx[\"<EOS>\"]:\n",
        "    break\n",
        "  model_input = torch.cat((model_input, predict_id))\n",
        "  prediction = model(predict_id)\n",
        "  predict_id = torch.tensor([torch.argmax(prediction[-1, :])])\n",
        "  predict_ids = torch.cat((predict_ids, predict_id))\n",
        "\n",
        "print(\"predict_ids\")\n",
        "for i in predict_ids:\n",
        "  print(idx_to_token[i.item()], end=\" \")"
      ],
      "metadata": {
        "id": "Zb0Wxh_riIyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39374e70-867e-48fc-be6e-667cfbaf1ff1"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict_ids\n",
            "the aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma aroma "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c--GgNasiIvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3kjRbgUiIr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwN3gi7WiIos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGv7YUhjiIly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kivXC8EGiIiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGyZCAoliIf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhOtFqUuiIdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEujylbQiIaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y10NLdrHiIXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7h_WX5BhiIVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qptliTyiISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4XeuhRiiIP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tmHIuugQiINX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQmyOIGdiIKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3DRGyAViIIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}